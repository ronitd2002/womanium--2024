{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51c5d5fe-20f8-41b9-a447-9664401ce144",
   "metadata": {},
   "source": [
    "## Fitting the Parity Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c979de87-ef44-4682-bd91-5b3fda605490",
   "metadata": {},
   "source": [
    "In this tutorial, we try to build a variational circuit that approximates the **parity fuction**:  \n",
    "$$f: x \\in \\{0,1 \\} ^{\\otimes n} \\rightarrow y = \\begin{cases}\n",
    "1 & \\text{if uneven number of 1's in $x$}\\\\\n",
    "0 & \\text{else}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b65480-f73c-4be2-bc5b-f7b130fdfca4",
   "metadata": {},
   "source": [
    "We encode the binary input values of $x$ as basis states at the initialization of the circuit. Since the encoded states are simply the computational basis states, this method is called **basis encoding**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7bf808f-c4cd-4bf7-95dc-8e375086db99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.optimize import NesterovMomentumOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b25c545-66ad-4f37-85bd-5820bef7f53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up device\n",
    "dev = qml.device(\"default.qubit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38299b39-f98b-48b3-bdb7-b8ab341a38b7",
   "metadata": {},
   "source": [
    "A variational circuit is typically constructed with a series of **layers**. Each layer consists of parametrized rotation gates operated on each qubit, followed by a ring of CNOT gates that entangle each qubit with its neighbour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f73cb23-9e17-4a35-bf2c-3c634782c865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(layer_weights):\n",
    "    for wire in range(4):\n",
    "        qml.Rot(*layer_weights[wire], wires=wire)\n",
    "\n",
    "    for wires in ([0, 1], [1, 2], [2, 3], [3, 0]):\n",
    "        qml.CNOT(wires)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a410ea5a-a122-4395-aa3c-4ccaea6d83db",
   "metadata": {},
   "source": [
    "Prepare basis state $\\Ket{x}$ using the `qml.BasisState()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2929964e-f6d1-411c-aee4-10912211f88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_preparation(x):\n",
    "    qml.BasisState(x, wires=[0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5dcf6f-35e5-4fa3-9a04-5465792cba07",
   "metadata": {},
   "source": [
    "Construct the circuit by first initializing the basis state, and then calling in parametrized layers. At the end, return the expected value of the Pauli operator $Z$ on the $0th$ qubit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc3de5d7-dcc3-48a4-88c6-0fbc172ec112",
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev)\n",
    "def circuit(weights, x):\n",
    "    state_preparation(x)\n",
    "\n",
    "    for layer_weights in weights:\n",
    "        layer(layer_weights)\n",
    "\n",
    "    return qml.expval(qml.PauliZ(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb06c7a-8901-4518-9a93-10e435a1d5cf",
   "metadata": {},
   "source": [
    "We add a \"classical\" bias to the expectation value obtained from the quantum circuit and return it. The circuit, along with the bias, is the variational classifier we wish to optimize. Given the binary input $x$, we want the variational classifier to return the value of the parity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40213e69-2dcc-4ac1-b79f-b3893f9df020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variational_classifier(weights, bias, x):\n",
    "    return circuit(weights, x) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfc1cd-06c1-4172-b5db-ed1bd23a13ec",
   "metadata": {},
   "source": [
    "Further, we define the accuracy metric and cost function. \n",
    "\n",
    "The cost function is defined as the mean of the square of the difference between the predicted values by the variational classifier and the target values (i.e., labels):\n",
    "$$\\text{Cost} = \\dfrac{1}{n}\\sum_{i=1}^{n}(\\text{Prediction}(i) - \\text{Label}(i))^2$$\n",
    "The accuracy metric is defined as\n",
    "$$\\text{Acc} = \\dfrac{1}{n}\\sum_{i=1}^{n}\\text{Bool}(\\text{label}(i) - \\text{prediction}(i)=0)$$\n",
    "Allowing for small computational errors, we set the condition inside the `bool` function to be {$<10^{-5}$} instead of {$=0$}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9b3aad3-7d45-4905-816d-271116dcdea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_loss(labels, predictions):\n",
    "    # We use a call to qml.math.stack to allow subtracting the arrays directly\n",
    "    return np.mean((labels - qml.math.stack(predictions)) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daad4d8e-88cf-4649-b6f5-a30b47352779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(labels, predictions):\n",
    "    acc = sum(abs(l - p) < 1e-5 for l, p in zip(labels, predictions))\n",
    "    acc = acc / len(labels)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caf4a6ba-360c-48e4-bd23-b5f6a790c78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(weights, bias, X, Y):\n",
    "    predictions = [variational_classifier(weights, bias, x) for x in X]\n",
    "    return square_loss(Y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c51b0aa-28b2-4196-861d-4d2f3584527d",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d054738e-df16-41fc-b531-bcf95fbdbcf4",
   "metadata": {},
   "source": [
    "Import data from `parity_train.txt` file and load it into `X` and `Y` arrays, shift `Y` labels from $\\{0,1\\}$ to $\\{-1,1\\}$, and print the function values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25514ced-88cb-43d1-80d1-8b885345bf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [0 0 0 1], y = 1\n",
      "x = [0 0 1 0], y = 1\n",
      "x = [0 1 0 0], y = 1\n",
      "x = [0 1 0 1], y = -1\n",
      "x = [0 1 1 0], y = -1\n",
      "x = [0 1 1 1], y = 1\n",
      "x = [1 0 0 0], y = 1\n",
      "x = [1 0 0 1], y = -1\n",
      "x = [1 0 1 1], y = 1\n",
      "x = [1 1 1 1], y = -1\n"
     ]
    }
   ],
   "source": [
    "# import data from parity_train.txt file and load it into X and Y arrays\n",
    "data = np.loadtxt(\"variational_classifier/data/parity_train.txt\", dtype=int)\n",
    "X = np.array(data[:, :-1])\n",
    "Y = np.array(data[:, -1])\n",
    "\n",
    "# shift label from {0, 1} to {-1, 1}\n",
    "Y = Y * 2 - 1\n",
    "\n",
    "# print function values\n",
    "for x,y in zip(X, Y):\n",
    "    print(f\"x = {x}, y = {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2087b919-8c8a-4c70-afad-ef336e6eb033",
   "metadata": {},
   "source": [
    "The state $\\Ket{x}$ is made of four binary digits, and thus we set the number of qubits to be four. We guess the number of layers sufficient to be two. We randomly initialize the weights and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c14965e4-031c-4e24-adbc-0497f08370af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [[[ 0.01764052  0.00400157  0.00978738]\n",
      "  [ 0.02240893  0.01867558 -0.00977278]\n",
      "  [ 0.00950088 -0.00151357 -0.00103219]\n",
      "  [ 0.00410599  0.00144044  0.01454274]]\n",
      "\n",
      " [[ 0.00761038  0.00121675  0.00443863]\n",
      "  [ 0.00333674  0.01494079 -0.00205158]\n",
      "  [ 0.00313068 -0.00854096 -0.0255299 ]\n",
      "  [ 0.00653619  0.00864436 -0.00742165]]]\n",
      "Bias:  0.0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "num_qubits = 4\n",
    "num_layers = 2\n",
    "weights_init = 0.01 * np.random.randn(num_layers, num_qubits, 3, requires_grad=True)\n",
    "bias_init = np.array(0.0, requires_grad=True)\n",
    "\n",
    "print(\"Weights:\", weights_init)\n",
    "print(\"Bias: \", bias_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83108e5b-3e8e-4ab7-9f3d-c5064c9be740",
   "metadata": {},
   "source": [
    "Define Optimizer and batch size \\\\\n",
    "Nesterov Momentum Optimizer is a gradient-descent optimizer. The batch size helps with randomly selecting a limited number of data points to calculate cost and optimize over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d9cb06c-7772-49a1-8c92-82eee53cf076",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = NesterovMomentumOptimizer(0.5)\n",
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1112610a-51c3-4fff-930b-c70afe17fb4d",
   "metadata": {},
   "source": [
    "Train the model over a hundred iterations. At each iteration, a random batch of data points is selected. The cost function with the parameters of the iteration is calculated, and gradient descent is applied and the next iteration of parameters is thus found trying to minimize the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd93e800-38e4-4157-8e01-12730f0a7654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:    1 | Cost: 2.3147651 | Accuracy: 0.5000000\n",
      "Iter:    2 | Cost: 1.9664866 | Accuracy: 0.5000000\n",
      "Iter:    3 | Cost: 1.9208589 | Accuracy: 0.5000000\n",
      "Iter:    4 | Cost: 2.6276126 | Accuracy: 0.5000000\n",
      "Iter:    5 | Cost: 0.9323119 | Accuracy: 0.6000000\n",
      "Iter:    6 | Cost: 1.1903549 | Accuracy: 0.5000000\n",
      "Iter:    7 | Cost: 2.0508989 | Accuracy: 0.4000000\n",
      "Iter:    8 | Cost: 1.1275531 | Accuracy: 0.6000000\n",
      "Iter:    9 | Cost: 1.1659803 | Accuracy: 0.6000000\n",
      "Iter:   10 | Cost: 1.1349618 | Accuracy: 0.6000000\n",
      "Iter:   11 | Cost: 0.9994063 | Accuracy: 0.6000000\n",
      "Iter:   12 | Cost: 1.0812559 | Accuracy: 0.6000000\n",
      "Iter:   13 | Cost: 1.2863155 | Accuracy: 0.6000000\n",
      "Iter:   14 | Cost: 2.2658259 | Accuracy: 0.4000000\n",
      "Iter:   15 | Cost: 1.1323724 | Accuracy: 0.6000000\n",
      "Iter:   16 | Cost: 1.3439737 | Accuracy: 0.8000000\n",
      "Iter:   17 | Cost: 2.0076168 | Accuracy: 0.6000000\n",
      "Iter:   18 | Cost: 1.2685760 | Accuracy: 0.5000000\n",
      "Iter:   19 | Cost: 1.6762475 | Accuracy: 0.5000000\n",
      "Iter:   20 | Cost: 1.1868237 | Accuracy: 0.6000000\n",
      "Iter:   21 | Cost: 1.4784687 | Accuracy: 0.6000000\n",
      "Iter:   22 | Cost: 1.4599473 | Accuracy: 0.6000000\n",
      "Iter:   23 | Cost: 0.9573269 | Accuracy: 0.6000000\n",
      "Iter:   24 | Cost: 1.1657424 | Accuracy: 0.5000000\n",
      "Iter:   25 | Cost: 1.0877087 | Accuracy: 0.4000000\n",
      "Iter:   26 | Cost: 1.1683687 | Accuracy: 0.6000000\n",
      "Iter:   27 | Cost: 2.1141689 | Accuracy: 0.6000000\n",
      "Iter:   28 | Cost: 1.0272966 | Accuracy: 0.5000000\n",
      "Iter:   29 | Cost: 0.9664085 | Accuracy: 0.5000000\n",
      "Iter:   30 | Cost: 1.1287654 | Accuracy: 0.6000000\n",
      "Iter:   31 | Cost: 1.4202360 | Accuracy: 0.4000000\n",
      "Iter:   32 | Cost: 1.1286000 | Accuracy: 0.5000000\n",
      "Iter:   33 | Cost: 1.9594333 | Accuracy: 0.4000000\n",
      "Iter:   34 | Cost: 1.2811832 | Accuracy: 0.4000000\n",
      "Iter:   35 | Cost: 0.8522775 | Accuracy: 0.7000000\n",
      "Iter:   36 | Cost: 1.4765281 | Accuracy: 0.6000000\n",
      "Iter:   37 | Cost: 0.9603287 | Accuracy: 0.6000000\n",
      "Iter:   38 | Cost: 1.6031314 | Accuracy: 0.6000000\n",
      "Iter:   39 | Cost: 1.1700888 | Accuracy: 0.4000000\n",
      "Iter:   40 | Cost: 1.7571779 | Accuracy: 0.4000000\n",
      "Iter:   41 | Cost: 1.9608116 | Accuracy: 0.6000000\n",
      "Iter:   42 | Cost: 2.0802752 | Accuracy: 0.6000000\n",
      "Iter:   43 | Cost: 1.1904884 | Accuracy: 0.3000000\n",
      "Iter:   44 | Cost: 0.9941585 | Accuracy: 0.6000000\n",
      "Iter:   45 | Cost: 1.0709609 | Accuracy: 0.5000000\n",
      "Iter:   46 | Cost: 0.9780625 | Accuracy: 0.6000000\n",
      "Iter:   47 | Cost: 1.1573709 | Accuracy: 0.6000000\n",
      "Iter:   48 | Cost: 1.0235239 | Accuracy: 0.6000000\n",
      "Iter:   49 | Cost: 1.2842469 | Accuracy: 0.5000000\n",
      "Iter:   50 | Cost: 0.8549226 | Accuracy: 0.6000000\n",
      "Iter:   51 | Cost: 0.5136787 | Accuracy: 1.0000000\n",
      "Iter:   52 | Cost: 0.2488031 | Accuracy: 1.0000000\n",
      "Iter:   53 | Cost: 0.0461277 | Accuracy: 1.0000000\n",
      "Iter:   54 | Cost: 0.0293518 | Accuracy: 1.0000000\n",
      "Iter:   55 | Cost: 0.0205454 | Accuracy: 1.0000000\n",
      "Iter:   56 | Cost: 0.0352514 | Accuracy: 1.0000000\n",
      "Iter:   57 | Cost: 0.0576767 | Accuracy: 1.0000000\n",
      "Iter:   58 | Cost: 0.0291305 | Accuracy: 1.0000000\n",
      "Iter:   59 | Cost: 0.0127137 | Accuracy: 1.0000000\n",
      "Iter:   60 | Cost: 0.0058108 | Accuracy: 1.0000000\n",
      "Iter:   61 | Cost: 0.0018002 | Accuracy: 1.0000000\n",
      "Iter:   62 | Cost: 0.0014089 | Accuracy: 1.0000000\n",
      "Iter:   63 | Cost: 0.0017489 | Accuracy: 1.0000000\n",
      "Iter:   64 | Cost: 0.0021282 | Accuracy: 1.0000000\n",
      "Iter:   65 | Cost: 0.0029876 | Accuracy: 1.0000000\n",
      "Iter:   66 | Cost: 0.0035331 | Accuracy: 1.0000000\n",
      "Iter:   67 | Cost: 0.0035540 | Accuracy: 1.0000000\n",
      "Iter:   68 | Cost: 0.0025639 | Accuracy: 1.0000000\n",
      "Iter:   69 | Cost: 0.0019459 | Accuracy: 1.0000000\n",
      "Iter:   70 | Cost: 0.0015856 | Accuracy: 1.0000000\n",
      "Iter:   71 | Cost: 0.0008439 | Accuracy: 1.0000000\n",
      "Iter:   72 | Cost: 0.0005960 | Accuracy: 1.0000000\n",
      "Iter:   73 | Cost: 0.0003122 | Accuracy: 1.0000000\n",
      "Iter:   74 | Cost: 0.0002446 | Accuracy: 1.0000000\n",
      "Iter:   75 | Cost: 0.0001745 | Accuracy: 1.0000000\n",
      "Iter:   76 | Cost: 0.0001215 | Accuracy: 1.0000000\n",
      "Iter:   77 | Cost: 0.0001141 | Accuracy: 1.0000000\n",
      "Iter:   78 | Cost: 0.0001538 | Accuracy: 1.0000000\n",
      "Iter:   79 | Cost: 0.0001871 | Accuracy: 1.0000000\n",
      "Iter:   80 | Cost: 0.0001330 | Accuracy: 1.0000000\n",
      "Iter:   81 | Cost: 0.0001380 | Accuracy: 1.0000000\n",
      "Iter:   82 | Cost: 0.0001336 | Accuracy: 1.0000000\n",
      "Iter:   83 | Cost: 0.0001483 | Accuracy: 1.0000000\n",
      "Iter:   84 | Cost: 0.0001234 | Accuracy: 1.0000000\n",
      "Iter:   85 | Cost: 0.0001359 | Accuracy: 1.0000000\n",
      "Iter:   86 | Cost: 0.0001268 | Accuracy: 1.0000000\n",
      "Iter:   87 | Cost: 0.0002270 | Accuracy: 1.0000000\n",
      "Iter:   88 | Cost: 0.0000865 | Accuracy: 1.0000000\n",
      "Iter:   89 | Cost: 0.0000774 | Accuracy: 1.0000000\n",
      "Iter:   90 | Cost: 0.0000759 | Accuracy: 1.0000000\n",
      "Iter:   91 | Cost: 0.0000607 | Accuracy: 1.0000000\n",
      "Iter:   92 | Cost: 0.0000523 | Accuracy: 1.0000000\n",
      "Iter:   93 | Cost: 0.0000536 | Accuracy: 1.0000000\n",
      "Iter:   94 | Cost: 0.0000444 | Accuracy: 1.0000000\n",
      "Iter:   95 | Cost: 0.0000384 | Accuracy: 1.0000000\n",
      "Iter:   96 | Cost: 0.0000497 | Accuracy: 1.0000000\n",
      "Iter:   97 | Cost: 0.0000263 | Accuracy: 1.0000000\n",
      "Iter:   98 | Cost: 0.0000229 | Accuracy: 1.0000000\n",
      "Iter:   99 | Cost: 0.0000339 | Accuracy: 1.0000000\n",
      "Iter:  100 | Cost: 0.0000174 | Accuracy: 1.0000000\n"
     ]
    }
   ],
   "source": [
    "weights = weights_init\n",
    "bias = bias_init\n",
    "for it in range(100):\n",
    "\n",
    "    # Update the weights by one optimizer step, using only a limited batch of data\n",
    "    batch_index = np.random.randint(0, len(X), (batch_size,))\n",
    "    X_batch = X[batch_index]\n",
    "    Y_batch = Y[batch_index]\n",
    "    weights, bias = opt.step(cost, weights, bias, X=X_batch, Y=Y_batch)\n",
    "\n",
    "    # Compute accuracy\n",
    "    predictions = [np.sign(variational_classifier(weights, bias, x)) for x in X]\n",
    "\n",
    "    current_cost = cost(weights, bias, X, Y)\n",
    "    acc = accuracy(Y, predictions)\n",
    "\n",
    "    print(f\"Iter: {it+1:4d} | Cost: {current_cost:0.7f} | Accuracy: {acc:0.7f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce52c457-87d3-4d6f-a0c4-e9702706c5a1",
   "metadata": {},
   "source": [
    "Lastly, we validate the trained model on test data. Sometimes models can overfit on the training data, and thus perform weakly over unseen data points. Thus, validation over test data is an important part of any ML (classical and quantum) pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c041abee-71c9-4810-8bec-dbdcd97a42f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [0 0 0 0], y = -1, pred=-1.0\n",
      "x = [0 0 1 1], y = -1, pred=-1.0\n",
      "x = [1 0 1 0], y = -1, pred=-1.0\n",
      "x = [1 1 1 0], y = 1, pred=1.0\n",
      "x = [1 1 0 0], y = -1, pred=-1.0\n",
      "x = [1 1 0 1], y = 1, pred=1.0\n",
      "Accuracy on unseen data: 1.0\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt(\"variational_classifier/data/parity_test.txt\", dtype=int)\n",
    "X_test = np.array(data[:, :-1])\n",
    "Y_test = np.array(data[:, -1])\n",
    "Y_test = Y_test * 2 - 1  # shift label from {0, 1} to {-1, 1}\n",
    "\n",
    "predictions_test = [np.sign(variational_classifier(weights, bias, x)) for x in X_test]\n",
    "\n",
    "for x,y,p in zip(X_test, Y_test, predictions_test):\n",
    "    print(f\"x = {x}, y = {y}, pred={p}\")\n",
    "\n",
    "acc_test = accuracy(Y_test, predictions_test)\n",
    "print(\"Accuracy on unseen data:\", acc_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
